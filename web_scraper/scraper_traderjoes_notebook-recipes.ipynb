{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "bff55003-d1f7-4aa2-bb30-365027c5b129",
   "metadata": {},
   "source": [
    "## Trader Joes Scraper Example ##\n",
    "\n",
    "The following notebook is to guide the user through the web scraping process for extracting prices and recipes from the grocery store, Trader Joe's."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "5673b127-97cc-4997-8a0d-ea3725a35055",
   "metadata": {},
   "outputs": [],
   "source": [
    "import requests\n",
    "from bs4 import BeautifulSoup\n",
    "import pandas as pd\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b2fa3034-e69e-4efc-9726-94c64d2d6d9e",
   "metadata": {},
   "outputs": [],
   "source": [
    "from selenium import webdriver\n",
    "from selenium.webdriver.chrome.service import Service\n",
    "from webdriver_manager.chrome import ChromeDriverManager\n",
    "from selenium.webdriver.common.by import By\n",
    "from selenium.webdriver.chrome.options import Options\n",
    "\n",
    "# Set up Chrome options\n",
    "chrome_options = Options()\n",
    "chrome_options.add_argument(\"--no-sandbox\")\n",
    "chrome_options.add_argument(\"--disable-dev-shm-usage\")\n",
    "chrome_options.add_argument(\"--window-size=1920,1080\")\n",
    "driver = webdriver.Chrome(service=Service(ChromeDriverManager().install()), options=chrome_options)\n",
    "\n",
    "driver.get(\"https://www.traderjoes.com/home/products/pdp/organic-milk-a2a2-080971\")\n",
    "\n",
    "driver.implicitly_wait(10)\n",
    "time.sleep(5)\n",
    "# Extract product details\n",
    "\n",
    "try:\n",
    "    product_title = driver.find_element(By.CSS_SELECTOR, \"h1.ProductDetails_main__title__14Cnm\").text\n",
    "    product_price = driver.find_element(By.CSS_SELECTOR, \"span.ProductPrice_productPrice__price__3-50j\").text\n",
    "\n",
    "    print(\"Product Title:\", product_title)\n",
    "    print(\"Price:\", product_price)\n",
    "\n",
    "except Exception as e:\n",
    "    print(\"Error extracting product details:\", e)\n",
    "\n",
    "nutrition_data = {}\n",
    "\n",
    "try:\n",
    "    # 1. Find the nutrition container\n",
    "    container = driver.find_element(By.CSS_SELECTOR, \"div.NutritionFacts_nutritionFacts__1Nvz0\")\n",
    "\n",
    "    # 2. Extract characteristics (serving size, calories, etc.)\n",
    "    characteristics = container.find_elements(By.CSS_SELECTOR, \"div.Item_characteristics__item__2TgL-\")\n",
    "    for item in characteristics:\n",
    "        title_elem = item.find_element(By.CSS_SELECTOR, \"div.Item_characteristics__title__7nfa8\")\n",
    "        try:\n",
    "            text_elem = item.find_element(By.CSS_SELECTOR, \"div.Item_characteristics__text__dcfEC\")\n",
    "            nutrition_data[title_elem.text] = text_elem.text\n",
    "        except:\n",
    "            nutrition_data[title_elem.text] = None\n",
    "\n",
    "    # 3. Extract table data\n",
    "    rows = container.find_elements(By.CSS_SELECTOR, \"table.Item_table__2PMbE tbody tr\")\n",
    "    for row in rows:\n",
    "        cells = row.find_elements(By.TAG_NAME, \"td\")\n",
    "        if len(cells) == 3:\n",
    "            nutrient = cells[0].text\n",
    "            amount = cells[1].text\n",
    "            dv = cells[2].text\n",
    "            nutrition_data[nutrient] = {\"amount\": amount, \"%dv\": dv}\n",
    "\n",
    "    # Print results\n",
    "    from pprint import pprint\n",
    "    pprint(nutrition_data)\n",
    "\n",
    "finally:\n",
    "    driver.quit()\n",
    "# Close the WebDriver\n",
    "driver.quit()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "56f0ef8b-4976-4179-a63a-d40a3a860000",
   "metadata": {},
   "outputs": [],
   "source": [
    "from selenium import webdriver\n",
    "from selenium.webdriver.chrome.service import Service\n",
    "from selenium.webdriver.chrome.options import Options\n",
    "from selenium.webdriver.common.by import By\n",
    "from selenium.webdriver.support.ui import WebDriverWait\n",
    "from selenium.webdriver.support import expected_conditions as EC\n",
    "from webdriver_manager.chrome import ChromeDriverManager\n",
    "from selenium.common.exceptions import TimeoutException, StaleElementReferenceException\n",
    "import pandas as pd\n",
    "import time\n",
    "import re\n",
    "\n",
    "def scrape_category(url):\n",
    "    chrome_options = Options()\n",
    "    #chrome_options.add_argument(\"--headless=new\")\n",
    "    chrome_options.add_argument(\"--disable-gpu\")\n",
    "    chrome_options.add_argument(\"--window-size=1920,1080\")\n",
    "    \n",
    "    driver = webdriver.Chrome(service=Service(ChromeDriverManager().install()), options=chrome_options)\n",
    "    driver.get(url)\n",
    "\n",
    "    all_products = []\n",
    "    page = 1\n",
    "    time.sleep(3)\n",
    "    while True:\n",
    "        print(f\"Scraping page {page}...\")\n",
    "        WebDriverWait(driver, 15).until(\n",
    "            EC.presence_of_all_elements_located((By.CSS_SELECTOR, \"section.ProductCard_card__4WAOg\"))\n",
    "        )\n",
    "    \n",
    "        # --- scrape cards (same as before) ---\n",
    "        cards = driver.find_elements(By.CSS_SELECTOR, \"section.ProductCard_card__4WAOg\")\n",
    "        for card in cards:\n",
    "            try:\n",
    "                name_elem = card.find_element(By.CSS_SELECTOR, \"h2 a\")\n",
    "                name = name_elem.text.strip()\n",
    "                link = name_elem.get_attribute(\"href\")\n",
    "            except:\n",
    "                name = None\n",
    "                link = None\n",
    "    \n",
    "            try:\n",
    "                price = card.find_element(By.CSS_SELECTOR, \"span.ProductPrice_productPrice__price__3-50j\").text.strip()\n",
    "            except:\n",
    "                price = None\n",
    "    \n",
    "            try:\n",
    "                unit = card.find_element(By.CSS_SELECTOR, \"span.ProductPrice_productPrice__unit__2jvkA\").text.strip()\n",
    "            except:\n",
    "                unit = None\n",
    "    \n",
    "            all_products.append({\n",
    "                \"name\": name, \"price\": price, \"unit\": unit, \"url\": link, \"page\": page\n",
    "            })\n",
    "    \n",
    "        # --- pagination handling ---\n",
    "\n",
    "        try:\n",
    "            current_page_elem = driver.find_element(\n",
    "                By.CSS_SELECTOR, \"li.PaginationItem_paginationItem_selected__3BZC-\"\n",
    "            )\n",
    "            current_page_text = current_page_elem.text.strip()\n",
    "        \n",
    "            # Try extracting digits from text or aria-label\n",
    "            match = re.search(r'\\d+', current_page_text)\n",
    "            if not match:\n",
    "                aria_label = current_page_elem.get_attribute(\"aria-label\") or \"\"\n",
    "                match = re.search(r'\\d+', aria_label)\n",
    "        \n",
    "            if match:\n",
    "                current_page = match.group()\n",
    "            else:\n",
    "                print(\"‚ö†Ô∏è Could not determine current page number, stopping.\")\n",
    "                break\n",
    "        \n",
    "            pagination_items = driver.find_elements(\n",
    "                By.CSS_SELECTOR, \"li.PaginationItem_paginationItem__2f87h\"\n",
    "            )\n",
    "        \n",
    "            next_item = None\n",
    "            for item in pagination_items:\n",
    "                label = item.text.strip() or (item.get_attribute(\"aria-label\") or \"\")\n",
    "                m = re.search(r'\\d+', label)\n",
    "                if m and m.group() == str(int(current_page) + 1):\n",
    "                    next_item = item\n",
    "                    break\n",
    "        \n",
    "            if not next_item:\n",
    "                print(\"‚úÖ No more pages to scrape.\")\n",
    "                break\n",
    "        \n",
    "            driver.execute_script(\"arguments[0].click();\", next_item)\n",
    "        \n",
    "            # Wait for the selected page to update visually\n",
    "            WebDriverWait(driver, 10).until(\n",
    "                EC.text_to_be_present_in_element(\n",
    "                    (By.CSS_SELECTOR, \"li.PaginationItem_paginationItem_selected__3BZC-\"),\n",
    "                    str(int(current_page) + 1)\n",
    "                )\n",
    "            )\n",
    "        \n",
    "            page += 1\n",
    "            time.sleep(1)\n",
    "        \n",
    "        except (TimeoutException, StaleElementReferenceException):\n",
    "            print(\"‚ö†Ô∏è Pagination ended or element went stale.\")\n",
    "            break\n",
    "\n",
    "    driver.quit()\n",
    "    return pd.DataFrame(all_products)\n",
    "\n",
    "\n",
    "# ---- Run it ----\n",
    "#url = \"https://www.traderjoes.com/home/products/category/meat-seafood-plant-based-122\"\n",
    "url = \"https://www.traderjoes.com/home/products/category/fresh-fruits-veggies-113\"\n",
    "df = scrape_category(url)\n",
    "\n",
    "# Preview results\n",
    "print(df.head())\n",
    "\n",
    "# Save locally\n",
    "df.to_csv(\"traderjoes_fresh-fruits-veggies_products.csv\", index=False)\n",
    "print(\"‚úÖ Saved traderjoes_fresh-fruits-veggies_products.csv\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "a465ba59-cdd3-4860-9b44-c33eb2f37023",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "üìÑ Scraping page 1...\n",
      "‚û°Ô∏è Going to page 2...\n",
      "üìÑ Scraping page 2...\n",
      "‚û°Ô∏è Going to page 3...\n",
      "üìÑ Scraping page 3...\n",
      "‚û°Ô∏è Going to page 4...\n",
      "üìÑ Scraping page 4...\n",
      "‚û°Ô∏è Going to page 5...\n",
      "üìÑ Scraping page 5...\n",
      "‚û°Ô∏è Going to page 6...\n",
      "üìÑ Scraping page 6...\n",
      "‚û°Ô∏è Going to page 7...\n",
      "üìÑ Scraping page 7...\n",
      "‚û°Ô∏è Going to page 8...\n",
      "üìÑ Scraping page 8...\n",
      "‚û°Ô∏è Going to page 9...\n",
      "üìÑ Scraping page 9...\n",
      "‚û°Ô∏è Going to page 10...\n",
      "üìÑ Scraping page 10...\n",
      "‚û°Ô∏è Going to page 11...\n",
      "üìÑ Scraping page 11...\n",
      "‚û°Ô∏è Going to page 12...\n",
      "üìÑ Scraping page 12...\n",
      "‚û°Ô∏è Going to page 13...\n",
      "üìÑ Scraping page 13...\n",
      "‚û°Ô∏è Going to page 14...\n",
      "üìÑ Scraping page 14...\n",
      "‚û°Ô∏è Going to page 15...\n",
      "üìÑ Scraping page 15...\n",
      "‚û°Ô∏è Going to page 16...\n",
      "üìÑ Scraping page 16...\n",
      "‚û°Ô∏è Going to page 17...\n",
      "üìÑ Scraping page 17...\n",
      "‚û°Ô∏è Going to page 18...\n",
      "üìÑ Scraping page 18...\n",
      "‚û°Ô∏è Going to page 19...\n",
      "üìÑ Scraping page 19...\n",
      "‚û°Ô∏è Going to page 20...\n",
      "üìÑ Scraping page 20...\n",
      "‚û°Ô∏è Going to page 21...\n",
      "üìÑ Scraping page 21...\n",
      "‚û°Ô∏è Going to page 22...\n",
      "üìÑ Scraping page 22...\n",
      "‚û°Ô∏è Going to page 23...\n",
      "üìÑ Scraping page 23...\n",
      "‚û°Ô∏è Going to page 24...\n",
      "üìÑ Scraping page 24...\n",
      "‚û°Ô∏è Going to page 25...\n",
      "üìÑ Scraping page 25...\n",
      "‚û°Ô∏è Going to page 26...\n",
      "üìÑ Scraping page 26...\n",
      "‚û°Ô∏è Going to page 27...\n",
      "üìÑ Scraping page 27...\n",
      "‚û°Ô∏è Going to page 28...\n",
      "üìÑ Scraping page 28...\n",
      "‚û°Ô∏è Going to page 29...\n",
      "üìÑ Scraping page 29...\n",
      "‚û°Ô∏è Going to page 30...\n",
      "üìÑ Scraping page 30...\n",
      "‚û°Ô∏è Going to page 31...\n",
      "üìÑ Scraping page 31...\n",
      "‚û°Ô∏è Going to page 32...\n",
      "üìÑ Scraping page 32...\n",
      "‚û°Ô∏è Going to page 33...\n",
      "üìÑ Scraping page 33...\n",
      "‚û°Ô∏è Going to page 34...\n",
      "üìÑ Scraping page 34...\n",
      "‚û°Ô∏è Going to page 35...\n",
      "üìÑ Scraping page 35...\n",
      "‚û°Ô∏è Going to page 36...\n",
      "üìÑ Scraping page 36...\n",
      "‚û°Ô∏è Going to page 37...\n",
      "üìÑ Scraping page 37...\n",
      "‚úÖ No next page found ‚Äî done!\n",
      "‚úÖ Saved recipes to trader_joes_recipes.csv\n"
     ]
    }
   ],
   "source": [
    "from selenium import webdriver\n",
    "from selenium.webdriver.common.by import By\n",
    "from selenium.webdriver.support.ui import WebDriverWait\n",
    "from selenium.webdriver.support import expected_conditions as EC\n",
    "import pandas as pd\n",
    "import time, re\n",
    "\n",
    "# ---- Selenium setup ----\n",
    "options = webdriver.ChromeOptions()\n",
    "options.add_argument(\"--start-maximized\")\n",
    "driver = webdriver.Chrome(options=options)\n",
    "\n",
    "BASE_URL = \"https://www.traderjoes.com\"\n",
    "RECIPES_URL = f\"{BASE_URL}/home/recipes\"\n",
    "\n",
    "# ---- Helper: scrape ingredients from a recipe in a new tab ----\n",
    "def scrape_ingredients_and_details(recipe_url):\n",
    "    serves = None\n",
    "    time_str = None\n",
    "    ingredients = []\n",
    "\n",
    "    try:\n",
    "        # Open in new tab\n",
    "        driver.execute_script(\"window.open(arguments[0], '_blank');\", recipe_url)\n",
    "        driver.switch_to.window(driver.window_handles[-1])\n",
    "\n",
    "        # Wait for ingredient list\n",
    "        WebDriverWait(driver, 10).until(\n",
    "            EC.presence_of_all_elements_located((By.CSS_SELECTOR, \"ul.IngredientsList_ingredientsList__1LoAJ li\"))\n",
    "        )\n",
    "\n",
    "        # --- Ingredients ---\n",
    "        ingredients_elems = driver.find_elements(By.CSS_SELECTOR, \"ul.IngredientsList_ingredientsList__1LoAJ li\")\n",
    "        ingredients = [i.text.strip() for i in ingredients_elems]\n",
    "\n",
    "        # --- Serves / Time ---\n",
    "        try:\n",
    "            meta_items = driver.find_elements(By.CSS_SELECTOR, \"span.RecipeDetails_recipeDetails__complexityItem__2X49n\")\n",
    "            for item in meta_items:\n",
    "                text = item.text.strip()\n",
    "                if text.lower().startswith(\"serves\"):\n",
    "                    serves = text\n",
    "                elif text.lower().startswith(\"time\"):\n",
    "                    time_str = text\n",
    "        except Exception as e:\n",
    "            print(f\"‚ö†Ô∏è Could not find serves/time info for {recipe_url}: {e}\")\n",
    "\n",
    "        time.sleep(1)\n",
    "\n",
    "    except Exception as e:\n",
    "        print(f\"‚ö†Ô∏è Could not scrape ingredients/details from {recipe_url}: {e}\")\n",
    "\n",
    "    finally:\n",
    "        # Close tab and return\n",
    "        driver.close()\n",
    "        driver.switch_to.window(driver.window_handles[0])\n",
    "        time.sleep(1)\n",
    "\n",
    "    return ingredients, serves, time_str\n",
    "\n",
    "# ---- Main: scrape all recipes ----\n",
    "def scrape_all_recipes():\n",
    "    driver.get(RECIPES_URL)\n",
    "    time.sleep(3)\n",
    "\n",
    "    recipes = []\n",
    "    page = 1\n",
    "\n",
    "    while True:\n",
    "        print(f\"üìÑ Scraping page {page}...\")\n",
    "\n",
    "        # Wait for recipe cards to load\n",
    "        WebDriverWait(driver, 10).until(\n",
    "            EC.presence_of_all_elements_located((By.CSS_SELECTOR, \"a.RecipeGridCard_recipe__1Wo__\"))\n",
    "        )\n",
    "\n",
    "        cards = driver.find_elements(By.CSS_SELECTOR, \"a.RecipeGridCard_recipe__1Wo__\")\n",
    "\n",
    "        for i in range(len(cards)):\n",
    "            # Re-fetch card each time in case the DOM changed\n",
    "            try:\n",
    "                cards = driver.find_elements(By.CSS_SELECTOR, \"a.RecipeGridCard_recipe__1Wo__\")\n",
    "                card = cards[i]\n",
    "            except IndexError:\n",
    "                print(f\"‚ö†Ô∏è Card index {i} out of range ‚Äî stopping early.\")\n",
    "                break\n",
    "        \n",
    "            # Extract title\n",
    "            try:\n",
    "                title = card.find_element(By.CSS_SELECTOR, \"h3.RecipeGridCard_recipe__title__3-8S-\").text.strip()\n",
    "            except:\n",
    "                title = None\n",
    "                print(f\"No title found for card {i}\")\n",
    "        \n",
    "            # Extract category\n",
    "            try:\n",
    "                category = card.find_element(By.CSS_SELECTOR, \"p.RecipeGridCard_recipe__categories__3b5AM\").text.strip()\n",
    "            except:\n",
    "                category = None\n",
    "        \n",
    "            # Extract link\n",
    "            try:\n",
    "                link = card.get_attribute(\"href\")\n",
    "                if link and link.startswith(\"/\"):\n",
    "                    link = BASE_URL + link\n",
    "            except:\n",
    "                link = None\n",
    "        \n",
    "            # Extract image\n",
    "            try:\n",
    "                img_elem = card.find_element(By.CSS_SELECTOR, \"div.RecipeGridCard_recipe__img__1hv4j img\")\n",
    "                img_url = img_elem.get_attribute(\"src\")\n",
    "            except:\n",
    "                img_url = None\n",
    "        \n",
    "            # Scrape ingredients from link\n",
    "            ingredients, serves, time_str = scrape_ingredients_and_details(link) if link else ([], None, None)\n",
    "        \n",
    "            recipes.append({\n",
    "                \"title\": title,\n",
    "                \"category\": category,\n",
    "                \"url\": link,\n",
    "                \"image_url\": img_url,\n",
    "                \"serves\": serves,\n",
    "                \"time\": time_str,\n",
    "                \"ingredients\": ingredients\n",
    "            })\n",
    "\n",
    "\n",
    "        # ---- Pagination ----\n",
    "        try:\n",
    "            time.sleep(2)\n",
    "            # Try to find a \"Next\" button first\n",
    "            next_buttons = driver.find_elements(By.CSS_SELECTOR, \"li.PaginationItem_paginationItem__2f87h button\")\n",
    "        \n",
    "            next_button = None\n",
    "            for btn in next_buttons:\n",
    "                aria_label = btn.get_attribute(\"aria-label\")\n",
    "                if aria_label and \"Next\" in aria_label:\n",
    "                    next_button = btn\n",
    "                    break\n",
    "        \n",
    "            if next_button:\n",
    "                driver.execute_script(\"arguments[0].click();\", next_button)\n",
    "                page += 1\n",
    "                print(f\"‚û°Ô∏è Going to page {page}...\")\n",
    "                time.sleep(3)\n",
    "                continue\n",
    "        \n",
    "            # Fallback: numbered pagination\n",
    "            pagination_items = driver.find_elements(By.CSS_SELECTOR, \"li.PaginationItem_paginationItem__2f87h\")\n",
    "            if not pagination_items:\n",
    "                print(\"‚úÖ No pagination found ‚Äî done!\")\n",
    "                break\n",
    "        \n",
    "            selected = None\n",
    "            for p in pagination_items:\n",
    "                classes = p.get_attribute(\"class\")\n",
    "                if \"PaginationItem_paginationItem_selected\" in classes:\n",
    "                    selected = p\n",
    "                    break\n",
    "        \n",
    "            # safer number extraction\n",
    "            text = selected.text.strip() if selected else \"\"\n",
    "            match = re.search(r\"\\d+\", text)\n",
    "            current_page_num = int(match.group()) if match else page\n",
    "            next_page_num = current_page_num + 1\n",
    "        \n",
    "            next_item = None\n",
    "            for item in pagination_items:\n",
    "                text = item.text.strip()\n",
    "                match = re.search(r\"\\d+\", text)\n",
    "                if match and int(match.group()) == next_page_num:\n",
    "                    next_item = item\n",
    "                    break\n",
    "        \n",
    "            if next_item:\n",
    "                driver.execute_script(\"arguments[0].click();\", next_item)\n",
    "                page += 1\n",
    "                print(f\"‚û°Ô∏è Going to page {page}...\")\n",
    "                time.sleep(3)\n",
    "            else:\n",
    "                print(\"‚úÖ No next page found ‚Äî done!\")\n",
    "                break\n",
    "        \n",
    "        except Exception as e:\n",
    "            print(f\"‚ö†Ô∏è Pagination ended: {e}\")\n",
    "            break\n",
    "\n",
    "    return pd.DataFrame(recipes)\n",
    "\n",
    "# ---- Run scraper ----\n",
    "recipes_df = scrape_all_recipes()\n",
    "\n",
    "# ---- Save to CSV ----\n",
    "recipes_df.to_csv(\"trader_joes_recipes.csv\", index=False)\n",
    "print(\"‚úÖ Saved recipes to trader_joes_recipes.csv\")\n",
    "\n",
    "driver.quit()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8ec9f4fe-d411-4668-9684-7914028ee9b4",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4be19086-bc6d-4e41-9818-d84f55715230",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
